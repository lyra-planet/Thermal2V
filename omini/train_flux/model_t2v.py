import lightning as L
from diffusers.pipelines import FluxPipeline
import torch
import wandb
import os
import yaml
import requests
import time
import logging
from typing import List, Optional, Dict, Any
import prodigyopt
from torch.utils.data import DataLoader, DistributedSampler
from peft import LoraConfig, get_peft_model_state_dict
from ..pipeline.flux_omini import transformer_forward, encode_images
from ..moe.mogle_t2v_unet import MoGLE  # å¯¼å…¥MoGLEæ¨¡å—
import warnings
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)




def check_loss_validity(loss: torch.Tensor) -> bool:
    """æ£€æŸ¥ loss æ˜¯å¦ä¸º NaN æˆ– Inf"""
    return not (torch.isnan(loss).any() or torch.isinf(loss).any())


def normalize_position_delta(p_delta: Any, default_value: tuple = (0, 0)) -> tuple:
    """
    ç»Ÿä¸€å¤„ç† position_delta æ ¼å¼
    æ”¯æŒï¼š[0, 0], [[0, 0]], (0, 0), ((0, 0))
    """
    if isinstance(p_delta, (list, tuple)):
        if len(p_delta) == 2 and isinstance(p_delta[0], (int, float)):
            return tuple(p_delta)
        elif len(p_delta) > 0 and isinstance(p_delta[0], (list, tuple)):
            return tuple(p_delta[0])
    return default_value

class OminiModel(L.LightningModule):
    """LoRA å¾®è°ƒ Flux Transformer çš„è®­ç»ƒæ¨¡å—ï¼Œé›†æˆMoGLEç‰¹å¾å¤„ç†"""
    
    def __init__(
        self,
        flux_pipe_id: str,
        lora_path: str = None,
        lora_config: dict = None,
        device: str = "cuda",
        dtype: torch.dtype = torch.bfloat16,
        model_config: dict = None,
        adapter_names: List[Optional[str]] = None,
        optimizer_config: dict = None,
        gradient_checkpointing: bool = False,
        max_sequence_length: int = 512,
        # MoGLE ç›¸å…³å‚æ•°
        use_mogle: bool = False,
        mogle_config: dict = None,
        condition_type: str = "thermal", 
    ):
        super().__init__()
        
        # é»˜è®¤å€¼å¤„ç†
        if model_config is None:
            model_config = {}
        if adapter_names is None:
            adapter_names = [None, None, "default"]
        if mogle_config is None:
            mogle_config = {}
        
        self.model_config = model_config
        self.optimizer_config = optimizer_config or {}
        self.adapter_names = adapter_names
        self.max_sequence_length = max_sequence_length
        self.use_mogle = use_mogle
        self.mogle_config = mogle_config
        self.condition_type = condition_type
        logger.info(f"Initializing OminiModel with adapter_names: {adapter_names}")
        logger.info(f"MoGLE enabled: {use_mogle}")

        # åŠ è½½é¢„è®­ç»ƒçš„ FluxPipeline
        logger.info(f"Loading FluxPipeline from {flux_pipe_id}")
        self.flux_pipe: FluxPipeline = FluxPipeline.from_pretrained(
            flux_pipe_id, torch_dtype=dtype
        ).to(device)
        self.transformer = self.flux_pipe.transformer
        self.transformer.gradient_checkpointing = gradient_checkpointing
        self.transformer.train()

        # å†»ç»“ä¸éœ€è¦è®­ç»ƒçš„æ¨¡å—
        self.flux_pipe.text_encoder.requires_grad_(False).eval()
        self.flux_pipe.text_encoder_2.requires_grad_(False).eval()
        self.flux_pipe.vae.requires_grad_(False).eval()

        # æ”¶é›†éœ€è¦è®­ç»ƒçš„ adapter åç§°
        self.adapter_set = set([each for each in adapter_names if each is not None])
        logger.info(f"Adapter set: {self.adapter_set}")

        # åˆå§‹åŒ– LoRA å±‚
        self.lora_layers = self.init_lora(lora_path, lora_config)
        logger.info(f"Initialized {len(self.lora_layers)} LoRA parameters")

        # åˆå§‹åŒ– MoGLE æ¨¡å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.mogle = None
        self.mogle_adapter_map = {}  # æ˜ å°„ adapter_name åˆ° MoGLE æ¨¡å—
        if use_mogle:
            self.init_mogle(mogle_config)
            logger.info(f"Initialized MoGLE with config: {mogle_config}")

        # è¿ç§»åˆ°è®¾å¤‡å’Œæ•°æ®ç±»å‹
        self.to(device).to(dtype)

        # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡
        self.log_loss = 0.0
        self.last_t = 0.0

    def init_mogle(self, mogle_config: dict):
        """åˆå§‹åŒ– MoGLE æ¨¡å—ï¼Œä¸ºæ¯ä¸ª adapter åˆ›å»ºä¸€ä¸ªç‹¬ç«‹å®ä¾‹"""
        if not mogle_config:
            mogle_config = {
                "input_dim": 64,
                "hidden_dim": 256,
                "output_dim": 64,
                "has_expert": True,
                "has_gating": True,
                "weight_is_scale": False,
            }
        
        logger.info(f"ğŸ”§ MoGLE Configuration: {mogle_config}")
        
        # ä¸ºæ¯ä¸ª adapter åˆ›å»ºä¸€ä¸ª MoGLE å®ä¾‹ï¼ˆæˆ–å…±ç”¨ä¸€ä¸ªï¼‰
        # è¿™é‡Œé‡‡ç”¨å…±ç”¨ç­–ç•¥ä»¥å‡å°‘å‚æ•°é‡
        self.mogle = MoGLE(
            input_dim=mogle_config.get("input_dim", 64),
            hidden_dim=mogle_config.get("hidden_dim", 256),
            output_dim=mogle_config.get("output_dim", 64),
        )
        self.mogle.train()
        
        logger.info(f"âœ“ Initialized shared MoGLE module")

    def init_lora(self, lora_path: str, lora_config: dict):
        # ç¡®ä¿è‡³å°‘æä¾›äº†è·¯å¾„æˆ–é…ç½®ä¹‹ä¸€
        assert lora_path or lora_config
        # å¦‚æœæä¾› lora_pathï¼Œè¡¨ç¤ºè¦åŠ è½½å·²æœ‰æƒé‡ï¼ˆæ­¤å¤„å°šæœªå®ç°ï¼‰
        if lora_path:
            # TODO: å®ç°ä» safetensors/ç›®å½•åŠ è½½ LoRA æƒé‡çš„é€»è¾‘
            raise NotImplementedError
        else:
            # å¦‚æœæ²¡æœ‰æƒé‡è·¯å¾„ï¼Œåˆ™ä¸º adapter_set ä¸­çš„æ¯ä¸ª adapter åˆ›å»º LoRA é…ç½®å¹¶æ³¨å†Œåˆ° transformer
            for adapter_name in self.adapter_set:
                self.transformer.add_adapter(
                    LoraConfig(**lora_config), adapter_name=adapter_name
                )
            lora_layers = filter(
                lambda p: p.requires_grad, self.transformer.parameters()
            )
        return list(lora_layers)

    def save_lora(self, path: str):
        """ä¿å­˜ LoRA æƒé‡å’Œ MoGLE æƒé‡"""
        os.makedirs(path, exist_ok=True)
        
        # ä¿å­˜ LoRA
        for adapter_name in self.adapter_set:
            FluxPipeline.save_lora_weights(
                save_directory=path,
                weight_name=f"{adapter_name}.safetensors",
                transformer_lora_layers=get_peft_model_state_dict(
                    self.transformer, adapter_name=adapter_name
                ),
                safe_serialization=True,
            )
        
        # ä¿å­˜ MoGLEï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_mogle and self.mogle is not None:
            torch.save(self.mogle.state_dict(), os.path.join(path, "mogle.pt"))
            logger.info(f"âœ“ Saved MoGLE checkpoint to {path}/mogle.pt")

    def load_mogle_checkpoint(self, mogle_path: str):
        """åŠ è½½ MoGLE checkpoint"""
        if not os.path.exists(mogle_path):
            raise FileNotFoundError(f"MoGLE checkpoint not found at {mogle_path}")
        
        if self.mogle is None:
            raise RuntimeError("MoGLE is not initialized. Set use_mogle=True in __init__")
        
        state_dict = torch.load(mogle_path, map_location=self.device)
        self.mogle.load_state_dict(state_dict)
        logger.info(f"âœ“ Loaded MoGLE checkpoint from {mogle_path}")

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        self.transformer.requires_grad_(False)
        opt_config = self.optimizer_config

        self.trainable_params = self.lora_layers.copy()
        
        # æ·»åŠ  MoGLE å‚æ•°åˆ°å¯è®­ç»ƒå‚æ•°åˆ—è¡¨
        if self.use_mogle and self.mogle is not None:
            self.trainable_params.extend(list(self.mogle.parameters()))
            logger.info(f"Added {sum(p.numel() for p in self.mogle.parameters())} MoGLE parameters to training")

        for p in self.trainable_params:
            p.requires_grad_(True)

        if opt_config.get("type") == "AdamW":
            optimizer = torch.optim.AdamW(
                self.trainable_params, **opt_config.get("params", {})
            )
        elif opt_config.get("type") == "Prodigy":
            optimizer = prodigyopt.Prodigy(
                self.trainable_params, **opt_config.get("params", {})
            )
        elif opt_config.get("type") == "SGD":
            optimizer = torch.optim.SGD(
                self.trainable_params, **opt_config.get("params", {})
            )
        else:
            raise NotImplementedError(f"Optimizer {opt_config.get('type')} not implemented")
        
        logger.info(f"Initialized {opt_config.get('type')} optimizer")
        return optimizer

    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        """å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        imgs, prompts = batch["image"], batch["description"]
        image_latent_mask = batch.get("image_latent_mask", None)

        # æ”¶é›†æ‰€æœ‰æ¡ä»¶
        conditions, position_deltas, position_scales, latent_masks = [], [], [], []
        for i in range(100):  # æ”¹ä¸ºæ›´åˆç†çš„ä¸Šé™
            if f"condition_{i}" not in batch:
                break
            conditions.append(batch[f"condition_{i}"])
            
            # æ ‡å‡†åŒ– position_delta
            raw_delta = batch.get(f"position_delta_{i}", [0, 0])
            position_deltas.append(normalize_position_delta(raw_delta))
            
            position_scales.append(batch.get(f"position_scale_{i}", [1.0])[0])
            latent_masks.append(batch.get(f"condition_latent_mask_{i}", None))

        with torch.no_grad():
            # ç¼–ç å›¾åƒ
            x_0, img_ids = encode_images(self.flux_pipe, imgs)
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", message=".*Token indices sequence length.*")
                warnings.filterwarnings("ignore", message=".*truncated because CLIP.*")
        
                # ç¼–ç æ–‡æœ¬
                (
                    prompt_embeds,
                    pooled_prompt_embeds,
                    text_ids,
                ) = self.flux_pipe.encode_prompt(
                    prompt=prompts,
                    prompt_2=None,
                    prompt_embeds=None,
                    pooled_prompt_embeds=None,
                    device=self.flux_pipe.device,
                    num_images_per_prompt=1,
                    max_sequence_length=self.max_sequence_length,
                    lora_scale=None,
                )

            # é‡‡æ ·æ—¶é—´æ­¥ä¸å™ªéŸ³
            t = torch.sigmoid(torch.randn((imgs.shape[0],), device=self.device))
            x_1 = torch.randn_like(x_0).to(self.device)
            t_ = t.unsqueeze(1).unsqueeze(1)
            x_t = ((1 - t_) * x_0 + t_ * x_1).to(self.dtype)
            
            if image_latent_mask is not None:
                x_0 = x_0[:, image_latent_mask[0]]
                x_1 = x_1[:, image_latent_mask[0]]
                x_t = x_t[:, image_latent_mask[0]]
                img_ids = img_ids[image_latent_mask[0]]

            # å¤„ç†æ¡ä»¶
            condition_latents, condition_ids = [], []
            for cond, p_delta, p_scale, latent_mask in zip(
                conditions, position_deltas, position_scales, latent_masks
            ):
                c_latents, c_ids = encode_images(self.flux_pipe, cond)
                
                # =============== MoGLE å¤„ç†ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰===============
                if self.use_mogle and self.mogle is not None:
                    c_latents = self.mogle.forward(
                        c_latents,  # [bs, 256, 64]
                        noise_latent=x_t,  # [bs, 256, 64]
                        timestep=t  # [bs,]
                    )  # è¾“å‡º: [bs, 256, 64]
                
                if p_scale != 1.0:
                    scale_bias = (p_scale - 1.0) / 2
                    c_ids[:, 1:] *= p_scale
                    c_ids[:, 1:] += scale_bias
                
                # åº”ç”¨ä½ç½®åç§»ï¼ˆå·²æ ‡å‡†åŒ–ä¸º tupleï¼‰
                c_ids[:, 1] += p_delta[0]
                c_ids[:, 2] += p_delta[1]
                
                if latent_mask is not None:
                    c_latents, c_ids = c_latents[latent_mask], c_ids[latent_mask[0]]
                
                condition_latents.append(c_latents)
                condition_ids.append(c_ids)

            guidance = (
                torch.ones_like(t).to(self.device)
                if self.transformer.config.guidance_embeds
                else None
            )

        # =============== æ„å»º group_mask ===============
        branch_n = 2 + len(conditions)
        group_mask = torch.ones([branch_n, branch_n], dtype=torch.bool).to(self.device)
        # Disable the attention cross different condition branches
        group_mask[2:, 2:] = torch.diag(torch.tensor([1] * len(conditions)))
        # Disable the attention from condition branches to image branch and text branch
        if self.model_config.get("independent_condition", False):
            group_mask[2:, :2] = False

        # =============== å‰å‘ä¼ æ’­ ===============
        transformer_out = transformer_forward(
            self.transformer,
            image_features=[x_t, *(condition_latents)],
            text_features=[prompt_embeds],
            img_ids=[img_ids, *(condition_ids)],
            txt_ids=[text_ids],
            timesteps=[t, t] + [torch.zeros_like(t)] * len(conditions),
            pooled_projections=[pooled_prompt_embeds] * branch_n,
            guidances=[guidance] * branch_n,
            adapters=self.adapter_names,
            return_dict=False,
            group_mask=group_mask,
        )
        pred = transformer_out[0]

        # =============== è®¡ç®—æŸå¤± ===============
        step_loss = torch.nn.functional.mse_loss(pred, (x_1 - x_0), reduction="mean")
        
        # æ£€æŸ¥ loss æœ‰æ•ˆæ€§
        if not check_loss_validity(step_loss):
            logger.warning(f"Invalid loss detected: {step_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        self.last_t = t.mean().item()

        # æŒ‡æ•°å¹³æ»‘è®°å½• loss
        self.log_loss = (
            step_loss.item()
            if self.log_loss == 0.0
            else self.log_loss * 0.95 + step_loss.item() * 0.05
        )
        
        return step_loss

