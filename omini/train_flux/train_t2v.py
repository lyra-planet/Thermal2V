import sys
sys.path.append("../../") 
import torch
from torch.utils.data import Dataset
import torchvision.transforms as T
import os
import random
import numpy as np
import re
from PIL import Image, ImageDraw, ImageFont

from datasets import load_dataset
from omini.train_flux.trainer_t2v import OminiModel, get_config, train
from omini.pipeline.flux_omini import Condition, convert_to_condition, generate

import json

# ==================== åŠ è½½é…ç½®å’Œåˆå§‹åŒ– ====================
ROOT = "/home/cunjian/kai/cache/T2V"
PROJECT_ROOT = f"{ROOT}/OminiControl"
CONFIG_PATH = f"{PROJECT_ROOT}/spatial_alignment_thermal.yaml"
config = get_config(CONFIG_PATH)
training_config = config["train"]
torch.cuda.set_device(int(os.environ.get("LOCAL_RANK", 0)))
description_file = os.path.join(training_config["dataset"]["root"], "train_descriptions.json")

# ==================== æ•°æ®é›†ç±» ====================

class ThermalToVisibleDataset(Dataset):
    def __init__(self, root_dir, description_file,
                 condition_size=(512, 512), target_size=(512, 512),
                 drop_text_prob=0.05, drop_image_prob=0.0):  # â† æ”¹: 0.1 â†’ 0.05
        self.trainA_dir = os.path.join(root_dir, "trainA")
        self.trainB_dir = os.path.join(root_dir, "trainB")
        self.condition_size = condition_size
        self.target_size = target_size
        self.drop_text_prob = drop_text_prob
        self.drop_image_prob = drop_image_prob

        # è¯»å– JSON æ–‡æœ¬æè¿°
        with open(description_file, "r", encoding="utf-8") as f:
            self.descriptions = json.load(f)
        
        # ========== æ–°å¢ï¼šæ•°æ®é›†è´¨é‡ç»Ÿè®¡ ==========
        self._print_dataset_stats()
        # =========================================
        
        # åˆ›å»ºçƒ­æˆåƒ-å¯è§å…‰å›¾åƒå¯¹
        self.pairs = []
        
        trainB_dict = {f: f for f in os.listdir(self.trainB_dir)}
        for fnameA in os.listdir(self.trainA_dir):
            keyA = fnameA
            if keyA in trainB_dict:
                thermal_path = os.path.join(self.trainA_dir, fnameA)
                visible_path = os.path.join(self.trainB_dir, trainB_dict[keyA])
                self.pairs.append((thermal_path, visible_path))
            else:
                print(f"[è­¦å‘Š] æ— å¯¹åº”å¯è§å…‰å›¾åƒ: {fnameA}")

        self.to_tensor = T.ToTensor()
        self._init_font()

    def _print_dataset_stats(self):
        """æ‰“å°æ•°æ®é›†è´¨é‡ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*70)
        print("ğŸ“Š æ•°æ®é›†è´¨é‡ç»Ÿè®¡")
        print("="*70)
        
        empty_count = 0
        short_count = 0
        long_count = 0
        token_lengths = []
        
        for key, desc in self.descriptions.items():
            if not desc or desc.strip() == "":
                empty_count += 1
                token_lengths.append(0)
            else:
                # ç²—ç•¥ä¼°è®¡ï¼š1 token â‰ˆ 1.3 characters
                token_est = len(desc) / 1.3
                token_lengths.append(token_est)
                
                if token_est < 5:
                    short_count += 1
                elif token_est > 77:
                    long_count += 1
        
        total = len(self.descriptions)
        avg_tokens = sum(token_lengths) / len(token_lengths) if token_lengths else 0
        avg_padding = max(0, 77 - avg_tokens)
        
        print(f"âœ“ æ€»æ ·æœ¬æ•°: {total}")
        print(f"âœ“ ç©ºæè¿°: {empty_count} ({100*empty_count/total:.1f}%)")
        print(f"âœ“ çŸ­æè¿° (<5 tokens): {short_count}")
        print(f"âœ“ é•¿æè¿° (>77 tokens): {long_count}")
        print(f"âœ“ å¹³å‡æè¿°é•¿åº¦: {avg_tokens:.1f} tokens")
        print(f"âœ“ å¹³å‡éœ€è¦å¡«å……: {avg_padding:.1f} tokens (endoftext)")
        print(f"âœ“ drop_text_prob: {self.drop_text_prob} (é¢å¤–ç©º prompt æ¯”ä¾‹)")
        
        estimated_empty = empty_count + int(total * self.drop_text_prob)
        estimated_padding_percent = (estimated_empty * 77 + (total - estimated_empty) * avg_padding) / (total * 77)
        print(f"âœ“ é¢„ä¼°å¡«å……æ¯”ä¾‹: {100*estimated_padding_percent:.1f}% (åŒ…å« drop_text)")
        print("="*70 + "\n")

    def _init_font(self):
        """åˆå§‹åŒ–å­—ä½“ï¼Œé¿å…é‡å¤åŠ è½½"""
        try:
            self.font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)
            self.font_small = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 16)
        except:
            try:
                self.font = ImageFont.truetype("C:\\Windows\\Fonts\\arial.ttf", 20)
                self.font_small = ImageFont.truetype("C:\\Windows\\Fonts\\arial.ttf", 16)
            except:
                self.font = ImageFont.load_default()
                self.font_small = ImageFont.load_default()

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        thermal_path, visible_path = self.pairs[idx]
    
        thermal_img = Image.open(thermal_path).convert("RGB").resize(self.condition_size, Image.Resampling.LANCZOS)
        visible_img = Image.open(visible_path).convert("RGB").resize(self.target_size, Image.Resampling.LANCZOS)
    
        rel_path = os.path.relpath(visible_path, self.trainB_dir)
        prompt = self.descriptions.get(rel_path, "")
        
        # ========== æ–°å¢ï¼šå¤„ç†ç©ºæè¿° ==========
        # å¦‚æœæè¿°ä¸ºç©ºï¼Œè®¾ç½®é»˜è®¤æè¿°ï¼ˆé™¤éæ•…æ„ drop_textï¼‰
        if (not prompt or prompt.strip() == ""):
            prompt = "a detailed face"
        # ======================================
        
        # ========== æ–°å¢ï¼šæˆªæ–­è¿‡é•¿æè¿° ==========
        # é˜²æ­¢ä¿¡æ¯ä¸¢å¤±å’Œè¿‡åº¦å¡«å……
        # MAX_PROMPT_LENGTH = 75  # ç•™ 2 ä¸ªç»™ EOS/special tokens
        # if len(prompt) > MAX_PROMPT_LENGTH * 1.3:  # ç²—ç•¥ä¼°è®¡ï¼š1.3 chars per token
        #     # åœ¨è¯è¾¹ç•Œæˆªæ–­ï¼Œé¿å…åœ¨å•è¯ä¸­é—´æ–­å¼€
        #     prompt = prompt[:int(MAX_PROMPT_LENGTH * 1.3)].rsplit(' ', 1)[0]
        # # ======================================
        
        # è®°å½•æ˜¯å¦ä¸¢å¼ƒäº† promptï¼Œç”¨äºä¸€è‡´æ€§å¤„ç†
        drop_text = random.random() < self.drop_text_prob
        drop_image = random.random() < self.drop_image_prob
        
        if drop_text:
            prompt = ""  # åªæœ‰åœ¨ drop_text æ—¶æ‰çœŸæ­£è®¾ä¸ºç©ºï¼ˆç”¨äºæ— æ¡ä»¶è®­ç»ƒï¼‰
        if drop_image:
            thermal_img = Image.new("RGB", self.condition_size, (0, 0, 0))
    
        return {
            "image": self.to_tensor(visible_img),
            "condition_0": self.to_tensor(thermal_img),
            "condition_type_0": "thermal",
            "position_delta_0": np.array([0, 0]),
            "description": prompt,
            "drop_text": drop_text,
            "drop_image": drop_image
        }


# ==================== åˆ›å»ºæ•°æ®é›† ====================

dataset = ThermalToVisibleDataset(
    root_dir=training_config["dataset"]["root"],
    description_file=description_file,
    condition_size=training_config["dataset"]["condition_size"],
    target_size=training_config["dataset"]["target_size"],
    drop_text_prob=training_config["dataset"]["drop_text_prob"],
    drop_image_prob=training_config["dataset"]["drop_image_prob"],
)

# éªŒè¯æ•°æ®é›†
print("æ•°æ®é›†å¤§å°:", len(dataset))
sample = dataset[0]
print("æ ·æœ¬é”®:", sample.keys())
print("å›¾åƒå¼ é‡å½¢çŠ¶:", sample["image"].shape)
print("æ¡ä»¶å›¾åƒå¼ é‡å½¢çŠ¶:", sample["condition_0"].shape)
print("æ¡ä»¶ç±»å‹:", sample["condition_type_0"])
print("æ–‡æœ¬æè¿°:", sample["description"])


# ==================== æ”¹è¿›çš„æµ‹è¯•å‡½æ•° ====================

@torch.no_grad()
def test_function(model, dataset, save_path, file_name, num_samples=3):
    """
    ä½¿ç”¨æ•°æ®é›†ä¸­éšæœºé€‰å–çš„çƒ­æˆåƒå›¾åƒ + prompt è¿›è¡Œæµ‹è¯•
    å¹¶ç”Ÿæˆå¹¶æ’å¯¹æ¯”å›¾åƒï¼ˆçƒ­æˆåƒ | ç›®æ ‡å›¾åƒ | ç”Ÿæˆå›¾åƒ + Promptï¼‰
    
    Args:
        model: OminiModel å®ä¾‹
        dataset: ThermalToVisibleDataset å®ä¾‹
        save_path: ä¿å­˜ç»“æœçš„ç›®å½•
        file_name: ä¿å­˜æ–‡ä»¶çš„å‰ç¼€å
        num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡ï¼Œé»˜è®¤ 3
    """
    condition_size = model.training_config["dataset"]["condition_size"]
    target_size = model.training_config["dataset"]["target_size"]
    position_delta = model.training_config["dataset"].get("position_delta", [0, 0])
    position_scale = model.training_config["dataset"].get("position_scale", 1.0)

    adapter = model.adapter_names[2]
    condition_type = model.training_config.get("condition_type", "thermal")
    
    # =============== åˆ›å»ºæ‰€æœ‰å¿…éœ€çš„å­ç›®å½• ===============
    subdirs = ["comparison", "thermal", "target", "generated"]
    for subdir in subdirs:
        subdir_path = os.path.join(save_path, subdir)
        os.makedirs(subdir_path, exist_ok=True)

    # ä»æ•°æ®é›†ä¸­éšæœºé€‰å–æ ·æœ¬
    dataset_size = len(dataset)
    sample_indices = random.sample(range(dataset_size), min(num_samples, dataset_size))

    from torchvision.transforms import ToPILImage
    to_pil = ToPILImage()

    for i, idx in enumerate(sample_indices):
        try:
            # ä»æ•°æ®é›†è·å–æ ·æœ¬
            sample = dataset[idx]
            thermal_tensor = sample["condition_0"]  # çƒ­æˆåƒ tensor
            target_tensor = sample["image"]  # ç›®æ ‡å¯è§å…‰ tensor
            prompt = sample["description"]
            drop_text = sample.get("drop_text", False)

            # å¦‚æœ prompt ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not prompt or prompt.strip() == "":
                if drop_text:
                    prompt = ""
                else:
                    prompt = "a detailed face"

            # è½¬æ¢ tensor åˆ° PIL Image
            thermal_img_pil = to_pil(thermal_tensor)
            target_img_pil = to_pil(target_tensor)

            # =============== ç”Ÿæˆå›¾åƒ ===============
            condition = Condition(
                thermal_img_pil,
                adapter,
                position_delta,
                position_scale
            )

            generator = torch.Generator(device=model.device)
            generator.manual_seed(42)  # ç»Ÿä¸€ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­

            res = generate(
                model.flux_pipe,
                prompt=prompt,
                conditions=[condition],
                height=target_size[1],
                width=target_size[0],
                generator=generator,
                model_config=model.model_config,
                kv_cache=model.model_config.get("independent_condition", False),
                mogle=model.mogle,
                guidance_scale=1.8, 
                image_guidance_scale=1.0,   
            )
            generated_img = res.images[0]

            # =============== ç”Ÿæˆå¹¶æ’å¯¹æ¯”å›¾åƒ ===============
            img_width, img_height = target_size[0], target_size[1]
            
            # è°ƒæ•´çƒ­æˆåƒå°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸
            thermal_img_resized = thermal_img_pil.resize((img_width, img_height), Image.Resampling.LANCZOS)
            
            # åˆ›å»ºæ°´å¹³æ’åˆ—çš„ç”»å¸ƒ
            canvas_width = img_width * 3 + 20  # ä¸‰å¼ å›¾åƒ + é—´éš”
            canvas_height = img_height + 100  # é¢å¤–ç©ºé—´æ”¾ prompt æ–‡å­—
            
            canvas = Image.new("RGB", (canvas_width, canvas_height), color="white")
            
            # ç²˜è´´ä¸‰å¼ å›¾åƒ
            canvas.paste(thermal_img_resized, (0, 0))
            canvas.paste(target_img_pil, (img_width + 10, 0))
            canvas.paste(generated_img, (img_width * 2 + 20, 0))
            
            # =============== æ·»åŠ æ–‡å­—æ ‡ç­¾å’Œ Prompt ===============
            draw = ImageDraw.Draw(canvas)
            
            # ä½¿ç”¨æ•°æ®é›†ä¸­é¢„åŠ è½½çš„å­—ä½“
            font = dataset.font
            font_small = dataset.font_small
            
            # æ·»åŠ åˆ—æ ‡é¢˜
            draw.text((img_width // 2 - 30, img_height + 10), "Thermal", fill="black", font=font)
            draw.text((img_width + img_width // 2 - 20, img_height + 10), "Target", fill="black", font=font)
            draw.text((img_width * 2 + img_width // 2 - 20, img_height + 10), "Generated", fill="black", font=font)
            
            # æ·»åŠ  Promptï¼ˆå¤„ç†é•¿æ–‡æœ¬æ¢è¡Œï¼‰
            prompt_y = img_height + 45
            max_chars_per_line = 80
            if len(prompt) > max_chars_per_line:
                prompt_lines = []
                for line_idx in range(0, len(prompt), max_chars_per_line):
                    prompt_lines.append(prompt[line_idx:line_idx + max_chars_per_line])
                for line_idx, line in enumerate(prompt_lines[:2]):  # æœ€å¤šæ˜¾ç¤ºä¸¤è¡Œ
                    draw.text((10, prompt_y + line_idx * 25), f"Prompt: {line}", fill="black", font=font_small)
            else:
                draw.text((10, prompt_y), f"Prompt: {prompt}", fill="black", font=font_small)

            # =============== ä¿å­˜å¯¹æ¯”å›¾åƒ ===============
            comparison_file_path = os.path.join(
                save_path, "comparison", f"{file_name}_{condition_type}_comparison_{i}.jpg"
            )
            canvas.save(comparison_file_path)
            print(f"âœ… å·²ä¿å­˜å¯¹æ¯”å›¾åƒ: {comparison_file_path}")
            
            # =============== åˆ†åˆ«ä¿å­˜ä¸‰å¼ åŸå§‹å›¾åƒ ===============
            thermal_file_path = os.path.join(
                save_path, "thermal", f"{file_name}_{condition_type}_thermal_{i}.jpg"
            )
            target_file_path = os.path.join(
                save_path, "target", f"{file_name}_{condition_type}_target_{i}.jpg"
            )
            generated_file_path = os.path.join(
                save_path, "generated", f"{file_name}_{condition_type}_generated_{i}.jpg"
            )
            
            thermal_img_resized.save(thermal_file_path)
            target_img_pil.save(target_file_path)
            generated_img.save(generated_file_path)
            
            print(f"   â”œâ”€ Thermal: {thermal_file_path}")
            print(f"   â”œâ”€ Target: {target_file_path}")
            print(f"   â”œâ”€ Generated: {generated_file_path}")
            print(f"   â””â”€ Prompt: {prompt}\n")

            # æ˜¾å¼é‡Šæ”¾å†…å­˜
            del thermal_img_pil, target_img_pil, generated_img, canvas, draw

        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ° (æ ·æœ¬ {i}): {e}\n")
        except RuntimeError as e:
            print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ (æ ·æœ¬ {i}): {e}\n")
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {i} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {type(e).__name__}: {e}\n")


# ==================== åˆ›å»ºæ¨¡å‹ ====================

# ========== å…³é”®å˜åŠ¨ 1: ä»é…ç½®ä¸­è¯»å– MoGLE é…ç½® ==========
use_mogle = config.get("use_mogle", False)
mogle_config = config.get("mogle_config", {}) if use_mogle else None

trainable_model = OminiModel(
    flux_pipe_id=config["flux_path"],
    lora_config=training_config["lora_config"],
    device=f"cuda",
    dtype=getattr(torch, config["dtype"]),
    optimizer_config=training_config["optimizer"],
    model_config=config.get("model", {}),
    gradient_checkpointing=training_config.get("gradient_checkpointing", False),
    # ========== å…³é”®å˜åŠ¨ 2: ä¼ å…¥ MoGLE ç›¸å…³å‚æ•° ==========
    use_mogle=use_mogle,
    mogle_config=mogle_config,
    condition_type=config.get("condition_type", "thermal"), 
)

# ========== å…³é”®å˜åŠ¨ 3: å¦‚æœæä¾›äº†ä¿å­˜çš„ MoGLE checkpointï¼Œåˆ™åŠ è½½ ==========
mogle_checkpoint = config.get("mogle_checkpoint_path", None)
if use_mogle and mogle_checkpoint and os.path.exists(mogle_checkpoint):
    print(f"ğŸ”§ Loading MoGLE checkpoint from {mogle_checkpoint}")
    trainable_model.load_mogle_checkpoint(mogle_checkpoint)


# ==================== å¼€å§‹è®­ç»ƒ ====================

train(dataset, trainable_model, config, test_function)